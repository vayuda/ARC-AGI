{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Our PHOG Using RandTransform and pretrained Image Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Need to add the root directory to the path in order to import source and image_encoder\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.append(root)\n",
    "\n",
    "from rand_transform import rand_transform, get_dsl_operations\n",
    "import source as source\n",
    "import image_encoder as encoder\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONDataset(Dataset):\n",
    "    def __init__(self, folder_path, only_inputs=True):\n",
    "        \"\"\"\n",
    "        Custom dataset to load in all json files within a target folder.\n",
    "        Returns a tuple of (key, ARC_Object).\n",
    "        \"\"\"\n",
    "        self.folder_path = os.path.join(root, folder_path)\n",
    "        self.only_inputs = only_inputs\n",
    "        self.samples = self.load_samples(self.folder_path)\n",
    "\n",
    "    def load_samples(self, folder_path):\n",
    "        samples = []\n",
    "        viable_keys = ['input'] if self.only_inputs else ['input', 'output']\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith('.json'):\n",
    "                file_path = os.path.join(folder_path, file_name)        \n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Iterate through 'train' and 'test' keys in the JSON\n",
    "                file_id = file_name.split('.')[0]\n",
    "                for split in ['train', 'test']:\n",
    "                    if split in data:\n",
    "                        for i, sample in enumerate(data[split]):\n",
    "                            for key in viable_keys:\n",
    "                                if key in sample:\n",
    "                                    image = np.array(sample[key], dtype=int)\n",
    "                                    arc_object = source.ARC_Object(image)\n",
    "                                    samples.append((f\"{file_id}-{key}-{i}\", arc_object))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "class PHOG_ARC_DatasetWrapper(Dataset):\n",
    "    def __init__(self, dataset, label_operations, rand_transform, elems_per_image=16, max_labels=8):\n",
    "        self.dataset = dataset\n",
    "        self.label_operations = {label_operation: i for i, label_operation in enumerate(label_operations)}\n",
    "        self.elems_per_image = elems_per_image\n",
    "        self.max_labels = max_labels\n",
    "        self.rand_transform = rand_transform\n",
    "        self.sep_token = \"<SEP>\"\n",
    "        self.pad_token = \"<PAD>\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key, image = self.dataset[idx]\n",
    "        use_base = random.random() < 0.2\n",
    "        # input_objs, transformed_objs, transforms = self.rand_transform(image, depth=5, use_base=use_base)\n",
    "        input_objs, transformed_objs, transforms = self.rand_transform(image, depth=3, use_base=True)\n",
    "        x = input_objs\n",
    "        x.extend([self.pad_token] * (self.elems_per_image - len(x)) + [self.sep_token])\n",
    "        x.extend(transformed_objs)\n",
    "        x.extend([self.pad_token] * ((self.elems_per_image*2 + 1) - len(x)))\n",
    "        y = list(map(lambda x: self.label_operations[x], transforms))\n",
    "        y.extend([-1] * (self.max_labels - len(y)))\n",
    "        return key, x, y\n",
    "    \n",
    "class PHOG_ARC_Dataloader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=True):\n",
    "        super().__init__(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=self.collate_fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        keys, x, y = zip(*batch)\n",
    "        keys = list(keys)\n",
    "        x_batch = list(x)\n",
    "        y_batch = list(y)\n",
    "        return keys, x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JSONDataset(\"data/training\", only_inputs=True)\n",
    "train_dataset = PHOG_ARC_DatasetWrapper(train_dataset, get_dsl_operations(), rand_transform, elems_per_image=16)\n",
    "train_loader = PHOG_ARC_Dataloader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Epoch: 1\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Epoch: 2\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Epoch: 3\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Epoch: 4\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Epoch: 5\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Epoch: 6\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Epoch: 7\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n",
      "Max attempts reached.\n"
     ]
    }
   ],
   "source": [
    "dsl_operations = get_dsl_operations()\n",
    "for j in range(10):\n",
    "    print(f\"Epoch: {j}\")\n",
    "    for i, (key, x, y) in enumerate(train_loader):\n",
    "        # print(f\"Key: {key}\")\n",
    "        # print(f\"X: {x}\")\n",
    "        # print(f\"Y: {y}\")\n",
    "        \n",
    "        # print(f\"Input Image:\\n{'='*50}\")\n",
    "        # for im in x[0]:\n",
    "        #     if im == \"<PAD>\":\n",
    "        #         continue\n",
    "        #     elif im == \"<SEP>\":\n",
    "        #         print(f\"Transformed Image:\\n{'='*50}\")\n",
    "        #         continue\n",
    "            \n",
    "            # im.plot_grid()\n",
    "            # print(im.grid.shape)\n",
    "        # print(f\"Y: {', '.join([dsl_operations[yi].__name__ if yi >= 0 else '' for yi in y[0]])}\")\n",
    "        # break\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-291c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
