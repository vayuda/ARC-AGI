{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Book for Our PCFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_17912\\1522931806.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import pcfg_models as pcfg_models\n",
    "import pcfg_loader as loader\n",
    "import source as source\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data yielder\n",
    "data_folder = 'training'\n",
    "loader_params = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': True,\n",
    "    'only_inputs': True,\n",
    "    'print_steps': False,\n",
    "    'moves_per_step': 1,\n",
    "    'max_steps': 1,\n",
    "    'p_use_base': 0.1,\n",
    "}\n",
    "data_yielder = loader.get_pcfg_datayielder(data_folder, loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\Desktop\\UCSD\\291C - Program Synthesis\\Project\\ARC-AGI\\image_encoder\\transformers\\vision_transformer.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Transformer instantiated with 84,064 parameters using Sinusoidal encodings.\n",
      "Vision Transformer instantiated with 84,064 parameters using Sinusoidal encodings.\n",
      "PCFG encoder instantiated with 52,288 parameters.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our Encoder classifier\n",
    "dsl_list = data_yielder.label_ops\n",
    "embedding_model_fp = 'vit_11-21-24_100k_vF.pth'\n",
    "em = source.embedding.load_ViT(embedding_model_fp, device='cpu')\n",
    "\n",
    "pcfg_encoder = pcfg_models.PCFG_Encoder(\n",
    "    n_embd = em.embed_dim, \n",
    "    n_head = 4, \n",
    "    n_layer = 6, \n",
    "    ff_hd = int(em.embed_dim * 2), \n",
    "    dropout = 0, # This may cause issues if you make this non-zero\n",
    "    block_size = 35, \n",
    "    dsl_mapping = dsl_list, \n",
    "    embedding_model_fp = embedding_model_fp,\n",
    "    freeze_emb_model=True, \n",
    "    device = device\n",
    ")\n",
    "pcfg_encoder = pcfg_encoder.to(device)\n",
    "em = None   # Clear the model from memory\n",
    "\n",
    "# Use this to load in a pre-trained model instead\n",
    "# filename = ?\n",
    "# pcfg_encoder = pcfg_models.PCFG_Encoder.load_model(f\"trained_pcfg_models/{filename}\", print_statements=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_input(input, pcfg_encoder, use_grads, device):\n",
    "    x = torch.zeros((len(input), pcfg_encoder.model_params['n_embd']), device=device)\n",
    "    special_tokens = pcfg_encoder.get_special_tokens(device)   # cls_dsl, cls_obj, pad, sep\n",
    "    \n",
    "    try:\n",
    "        first_pad = input[:16].index('<PAD>')\n",
    "    except ValueError:\n",
    "        first_pad = 16\n",
    "    \n",
    "    for i, obj in enumerate(input):\n",
    "        if obj == \"<PAD>\":\n",
    "            x[i, :] = special_tokens[2, :]\n",
    "        elif obj == \"<SEP>\":\n",
    "            x[i, :] = special_tokens[3, :]\n",
    "        elif isinstance(obj, source.ARC_Object):\n",
    "            # x[i, :] = torch.zeros((1, 64), device=device)\n",
    "            obj.set_embedding(pcfg_encoder.embedding_model, use_grads=use_grads)\n",
    "            x[i, :] = obj.embedding.to(device)\n",
    "        else:\n",
    "            raise NameError(\"Input contains object that is not '<PAD>', '<SEP>', or an ARC_Object.\")\n",
    "    x = torch.cat((special_tokens[:2, :], x), dim=0)\n",
    "    return x, first_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_loss(dsl_cls, obj_att, dsl_label, obj_label, temp=0.2):\n",
    "    \"\"\"\n",
    "    Compute multi-label multi-class loss using KL divergence.\n",
    "\n",
    "    Args:\n",
    "        dsl_cls: Tensor of size (len(dsl)) - predicted logits for DSL classes.\n",
    "        obj_att: Tensor of size (len(obj_att)) - predicted logits for object attention.\n",
    "        dsl_label: Tensor of integers up to len(dsl), representing ground truth labels for DSL classes.\n",
    "        obj_label: Tensor of integers up to len(obj_att), representing ground truth labels for object attention.\n",
    "\n",
    "    Returns:\n",
    "        Total loss as a scalar tensor.\n",
    "    \"\"\"\n",
    "    # One-hot encode labels for multi-label setup\n",
    "    dsl_target = torch.zeros_like(dsl_cls, dtype=torch.float)\n",
    "    dsl_target[dsl_label] = 1.0\n",
    "    dsl_target = dsl_target / dsl_target.sum(dim=-1, keepdim=True)  # Normalize to probabilities\n",
    "    obj_target = torch.zeros_like(obj_att, dtype=torch.float)\n",
    "    obj_target[obj_label] = 1.0\n",
    "    obj_target = obj_target / obj_target.sum(dim=-1, keepdim=True)  # Normalize to probabilities\n",
    "\n",
    "    # Convert predictions to log-probabilities\n",
    "    dsl_log_prob = F.log_softmax(dsl_cls, dim=-1)\n",
    "    obj_log_prob = F.log_softmax(obj_att, dim=-1)\n",
    "\n",
    "    # Compute KL divergence loss\n",
    "    dsl_loss = F.kl_div(dsl_log_prob, dsl_target, reduction='batchmean')  # log_target=False by default\n",
    "    obj_loss = F.kl_div(obj_log_prob, obj_target, reduction='batchmean')\n",
    "\n",
    "    return dsl_loss, obj_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/ 1] -  100: Total Loss = 0.2404, DSL Loss = 0.2404, Obj Loss = 0.3253\n",
      "[ 1/ 1] -  200: Total Loss = 0.2418, DSL Loss = 0.2418, Obj Loss = 0.3256\n",
      "[ 1/ 1] -  300: Total Loss = 0.2414, DSL Loss = 0.2414, Obj Loss = 0.3279\n",
      "[ 1/ 1] -  400: Total Loss = 0.2389, DSL Loss = 0.2389, Obj Loss = 0.3276\n",
      "[ 1/ 1] -  500: Total Loss = 0.2400, DSL Loss = 0.2400, Obj Loss = 0.3197\n",
      "[ 1/ 1] -  600: Total Loss = 0.2412, DSL Loss = 0.2412, Obj Loss = 0.3248\n",
      "[ 1/ 1] -  700: Total Loss = 0.2435, DSL Loss = 0.2435, Obj Loss = 0.3263\n",
      "[ 1/ 1] -  800: Total Loss = 0.2338, DSL Loss = 0.2338, Obj Loss = 0.3264\n",
      "[ 1/ 1] -  900: Total Loss = 0.2373, DSL Loss = 0.2373, Obj Loss = 0.3316\n",
      "[ 1/ 1] - 1000: Total Loss = 0.2394, DSL Loss = 0.2394, Obj Loss = 0.3227\n",
      "[ 1/ 1] - 1100: Total Loss = 0.2342, DSL Loss = 0.2342, Obj Loss = 0.3243\n",
      "[ 1/ 1] - 1200: Total Loss = 0.2402, DSL Loss = 0.2402, Obj Loss = 0.3179\n",
      "[ 1/ 1] - 1300: Total Loss = 0.2350, DSL Loss = 0.2350, Obj Loss = 0.3268\n",
      "[ 1/ 1] - 1400: Total Loss = 0.2451, DSL Loss = 0.2451, Obj Loss = 0.3279\n",
      "[ 1/ 1] - 1500: Total Loss = 0.2374, DSL Loss = 0.2374, Obj Loss = 0.3240\n",
      "[ 1/ 1] - 1600: Total Loss = 0.2378, DSL Loss = 0.2378, Obj Loss = 0.3306\n",
      "[ 1/ 1] - 1700: Total Loss = 0.2373, DSL Loss = 0.2373, Obj Loss = 0.3217\n"
     ]
    }
   ],
   "source": [
    "# Loop through data\n",
    "train_params = {\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 1e-4,\n",
    "    \"print_frequency\": 100,\n",
    "}\n",
    "\n",
    "optim = torch.optim.AdamW(pcfg_encoder.parameters(), lr=train_params['lr'])\n",
    "avg_losses = []\n",
    "seen_labels = [0] * len(dsl_list)\n",
    "dsl_cls = None\n",
    "obj_att = None\n",
    "\n",
    "for epoch in range(train_params['epochs']):\n",
    "    active = True\n",
    "    step, dsl_loss_sum, obj_loss_sum, total_loss_sum = 0, 0, 0, 0\n",
    "\n",
    "    while active:\n",
    "        try:\n",
    "            key, input, label, obj_idx = next(data_yielder)\n",
    "        except StopIteration:\n",
    "            active = False\n",
    "            break\n",
    "        except loader.SampleExhausted:\n",
    "            continue\n",
    "\n",
    "        seen_labels[label[0]] += 1\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            x, first_pad = embed_input(input, pcfg_encoder, use_grads=not pcfg_encoder.freeze_emb_model, device=device)\n",
    "            dsl_cls, obj_att = pcfg_encoder(x)\n",
    "            obj_att = obj_att[2:first_pad + 2]  # Since we have 2 special cls tokens at the start\n",
    "\n",
    "            # Compute individual losses\n",
    "            dsl_loss, obj_loss = compute_kl_loss(\n",
    "                dsl_cls, \n",
    "                obj_att, \n",
    "                torch.tensor(label, device=device), \n",
    "                torch.tensor(obj_idx, device=device)\n",
    "            )\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            # total_loss = dsl_loss + obj_loss\n",
    "            total_loss = dsl_loss\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "\n",
    "        # Accumulate losses\n",
    "        total_loss_sum += total_loss.item()\n",
    "        dsl_loss_sum += dsl_loss.item()\n",
    "        obj_loss_sum += obj_loss.item()\n",
    "        step += 1\n",
    "\n",
    "        if step % train_params['print_frequency'] == 0:\n",
    "            avg_total_loss = total_loss_sum / train_params['print_frequency']\n",
    "            avg_dsl_loss = dsl_loss_sum / train_params['print_frequency']\n",
    "            avg_obj_loss = obj_loss_sum / train_params['print_frequency']\n",
    "\n",
    "            avg_losses.append((avg_total_loss, avg_dsl_loss, avg_obj_loss))\n",
    "\n",
    "            print(f\"[{(epoch+1):>2}/{train_params['epochs']:>2}] - {step:>4}: Total Loss = {avg_total_loss:.4f}, DSL Loss = {avg_dsl_loss:.4f}, Obj Loss = {avg_obj_loss:.4f}\")\n",
    "\n",
    "            total_loss_sum = 0\n",
    "            dsl_loss_sum = 0\n",
    "            obj_loss_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0925e-01, -1.7139e-01, -3.2324e-01,  6.4754e-04, -6.7822e-01,\n",
      "        -2.5284e-02, -9.2712e-02,  2.6367e-01,  5.4785e-01], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.1008, 0.0948, 0.0814, 0.1125, 0.0571, 0.1097, 0.1025, 0.1465, 0.1946],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.0370, 0.1039, 0.1280, 0.1023], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Model and embedding model saved to trained_pcfg_models\\pcfg_encoder_20241203_165657.pth\n"
     ]
    }
   ],
   "source": [
    "print(dsl_cls)\n",
    "print(F.softmax(dsl_cls, dim=-1))\n",
    "print(obj_att)\n",
    "pcfg_encoder.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-291c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
