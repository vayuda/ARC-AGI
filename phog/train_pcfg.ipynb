{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Book for Our PCFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_10904\\1522931806.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import pcfg_models as pcfg_models\n",
    "import pcfg_loader as loader\n",
    "import source as source\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_params = {\n",
    "    'datafolder': 'training',\n",
    "    'batch_size': 1,\n",
    "    'shuffle': True,\n",
    "    'only_inputs': True,\n",
    "    'print_steps': False,\n",
    "    'moves_per_step': 1,\n",
    "    'max_steps': 1,\n",
    "    'p_use_base': 0.1,\n",
    "}\n",
    "\n",
    "data_yielder = loader.get_pcfg_datayielder(**loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\Desktop\\UCSD\\291C - Program Synthesis\\Project\\ARC-AGI\\image_encoder\\v2_trainer\\vision_transformer_v2.py:222: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Transformer instantiated with 117,472 parameters using Sinusoidal encodings.\n",
      "Vision Transformer instantiated with 117,472 parameters using Sinusoidal encodings.\n",
      "PCFG encoder instantiated with 52,288 parameters.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our Encoder classifier\n",
    "dsl_list = data_yielder.label_ops\n",
    "embedding_model_fp = 'vit_12-3-24_100k_v2.pth'\n",
    "em = source.embedding.load_v2_ViT(embedding_model_fp, device='cpu')\n",
    "\n",
    "pcfg_encoder = pcfg_models.PCFG_Encoder(\n",
    "    n_embd = em.embed_dim, \n",
    "    n_head = 4, \n",
    "    n_layer = 6, \n",
    "    ff_hd = int(em.embed_dim * 2), \n",
    "    dropout = 0, # This may cause issues if you make this non-zero\n",
    "    block_size = 35, \n",
    "    dsl_mapping = dsl_list, \n",
    "    embedding_model_fp = embedding_model_fp,\n",
    "    freeze_emb_model=True, \n",
    "    device = device\n",
    ")\n",
    "pcfg_encoder = pcfg_encoder.to(device)\n",
    "em = None   # Clear the model from memory\n",
    "\n",
    "# Use this to load in a pre-trained model instead\n",
    "# filename = ?\n",
    "# pcfg_encoder = pcfg_models.PCFG_Encoder.load_model(f\"trained_pcfg_models/{filename}\", print_statements=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_input(input, pcfg_encoder, use_grads, device):\n",
    "    x = torch.zeros((len(input), pcfg_encoder.model_params['n_embd']), device=device)\n",
    "    special_tokens = pcfg_encoder.get_special_tokens(device)   # cls_dsl, cls_obj, pad, sep\n",
    "    \n",
    "    try:\n",
    "        first_pad = input[:16].index('<PAD>')\n",
    "    except ValueError:\n",
    "        first_pad = 16\n",
    "    \n",
    "    for i, obj in enumerate(input):\n",
    "        if obj == \"<PAD>\":\n",
    "            x[i, :] = special_tokens[2, :]\n",
    "        elif obj == \"<SEP>\":\n",
    "            x[i, :] = special_tokens[3, :]\n",
    "        elif isinstance(obj, source.ARC_Object):\n",
    "            # x[i, :] = torch.zeros((1, 64), device=device)\n",
    "            obj.set_embedding(pcfg_encoder.embedding_model, use_grads=use_grads)\n",
    "            x[i, :] = obj.embedding.to(device)\n",
    "        else:\n",
    "            raise NameError(\"Input contains object that is not '<PAD>', '<SEP>', or an ARC_Object.\")\n",
    "    x = torch.cat((special_tokens[:2, :], x), dim=0)\n",
    "    return x, first_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_loss(dsl_cls, obj_att, dsl_label, obj_label, temp=0.2):\n",
    "    \"\"\"\n",
    "    Compute multi-label multi-class loss using KL divergence.\n",
    "\n",
    "    Args:\n",
    "        dsl_cls: Tensor of size (len(dsl)) - predicted logits for DSL classes.\n",
    "        obj_att: Tensor of size (len(obj_att)) - predicted logits for object attention.\n",
    "        dsl_label: Tensor of integers up to len(dsl), representing ground truth labels for DSL classes.\n",
    "        obj_label: Tensor of integers up to len(obj_att), representing ground truth labels for object attention.\n",
    "\n",
    "    Returns:\n",
    "        Total loss as a scalar tensor.\n",
    "    \"\"\"\n",
    "    # One-hot encode labels for multi-label setup\n",
    "    dsl_target = torch.zeros_like(dsl_cls, dtype=torch.float)\n",
    "    dsl_target[dsl_label] = 1.0\n",
    "    dsl_target = dsl_target / dsl_target.sum(dim=-1, keepdim=True)  # Normalize to probabilities\n",
    "    obj_target = torch.zeros_like(obj_att, dtype=torch.float)\n",
    "    obj_target[obj_label] = 1.0\n",
    "    obj_target = obj_target / obj_target.sum(dim=-1, keepdim=True)  # Normalize to probabilities\n",
    "\n",
    "    # Convert predictions to log-probabilities\n",
    "    dsl_log_prob = F.log_softmax(dsl_cls, dim=-1)\n",
    "    obj_log_prob = F.log_softmax(obj_att, dim=-1)\n",
    "\n",
    "    # Compute KL divergence loss\n",
    "    dsl_loss = F.kl_div(dsl_log_prob, dsl_target, reduction='batchmean')  # log_target=False by default\n",
    "    obj_loss = F.kl_div(obj_log_prob, obj_target, reduction='batchmean')\n",
    "\n",
    "    return dsl_loss, obj_loss\n",
    "\n",
    "def compute_cross_entropy_loss(dsl_cls, obj_att, dsl_label, obj_label, max_loss=2.5):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss for DSL and object attention logits, with clipping.\n",
    "\n",
    "    Args:\n",
    "        dsl_cls: Tensor of size (len(dsl)) - predicted logits for DSL classes.\n",
    "        obj_att: Tensor of size (len(obj_att)) - predicted logits for object attention.\n",
    "        dsl_label: Single element tensor representing the ground truth label index for DSL classes.\n",
    "        obj_label: Single element tensor representing the ground truth label index for object attention.\n",
    "        max_loss: Maximum value to clip the loss (default: 3.0).\n",
    "\n",
    "    Returns:\n",
    "        Clipped cross-entropy losses for DSL and object attention as scalars.\n",
    "    \"\"\"\n",
    "    # Compute cross-entropy loss directly\n",
    "    dsl_loss = F.cross_entropy(dsl_cls.unsqueeze(0), dsl_label)\n",
    "    obj_loss = F.cross_entropy(obj_att.unsqueeze(0), obj_label)\n",
    "\n",
    "    # Clip the loss to a maximum value\n",
    "    dsl_loss_clipped = torch.clamp(dsl_loss, max=max_loss)\n",
    "    obj_loss_clipped = torch.clamp(obj_loss, max=max_loss)\n",
    "\n",
    "    return dsl_loss_clipped, obj_loss_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/ 1] -  100: Total Loss = 2.1754, DSL Loss = 2.1754, Obj Loss = 1.4404\n",
      "[ 1/ 1] -  200: Total Loss = 2.1386, DSL Loss = 2.1386, Obj Loss = 1.5174\n",
      "[ 1/ 1] -  300: Total Loss = 2.1451, DSL Loss = 2.1451, Obj Loss = 1.4750\n",
      "[ 1/ 1] -  400: Total Loss = 2.1220, DSL Loss = 2.1220, Obj Loss = 1.5423\n",
      "[ 1/ 1] -  500: Total Loss = 2.1291, DSL Loss = 2.1291, Obj Loss = 1.5546\n",
      "[ 1/ 1] -  600: Total Loss = 2.1219, DSL Loss = 2.1219, Obj Loss = 1.6306\n",
      "[ 1/ 1] -  700: Total Loss = 2.0601, DSL Loss = 2.0601, Obj Loss = 1.4783\n",
      "[ 1/ 1] -  800: Total Loss = 2.0756, DSL Loss = 2.0756, Obj Loss = 1.5714\n",
      "[ 1/ 1] -  900: Total Loss = 2.0711, DSL Loss = 2.0711, Obj Loss = 1.4920\n",
      "[ 1/ 1] - 1000: Total Loss = 2.0955, DSL Loss = 2.0955, Obj Loss = 1.4747\n",
      "[ 1/ 1] - 1100: Total Loss = 2.0084, DSL Loss = 2.0084, Obj Loss = 1.5276\n",
      "[ 1/ 1] - 1200: Total Loss = 2.0471, DSL Loss = 2.0471, Obj Loss = 1.4302\n",
      "[ 1/ 1] - 1300: Total Loss = 2.0184, DSL Loss = 2.0184, Obj Loss = 1.5320\n",
      "[ 1/ 1] - 1400: Total Loss = 1.9928, DSL Loss = 1.9928, Obj Loss = 1.4602\n",
      "[ 1/ 1] - 1500: Total Loss = 2.0781, DSL Loss = 2.0781, Obj Loss = 1.5559\n",
      "[ 1/ 1] - 1600: Total Loss = 2.0424, DSL Loss = 2.0424, Obj Loss = 1.5565\n",
      "[ 1/ 1] - 1700: Total Loss = 1.9969, DSL Loss = 1.9969, Obj Loss = 1.4946\n"
     ]
    }
   ],
   "source": [
    "# Loop through data\n",
    "train_params = {\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 1e-4,\n",
    "    \"print_frequency\": 100,\n",
    "}\n",
    "\n",
    "optim = torch.optim.AdamW(pcfg_encoder.parameters(), lr=train_params['lr'])\n",
    "avg_losses = []\n",
    "dsl_cls = None\n",
    "obj_att = None\n",
    "itof = {v: k for k, v in dsl_list.items()}\n",
    "\n",
    "for epoch in range(train_params['epochs']):\n",
    "    active = True\n",
    "    step, dsl_loss_sum, obj_loss_sum, total_loss_sum = 0, 0, 0, 0\n",
    "\n",
    "    while active:\n",
    "        try:\n",
    "            key, input, label, obj_idx = next(data_yielder)\n",
    "        except StopIteration:\n",
    "            active = False\n",
    "            break\n",
    "        except loader.SampleExhausted:\n",
    "            continue\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            x, first_pad = embed_input(input, pcfg_encoder, use_grads=not pcfg_encoder.freeze_emb_model, device=device)\n",
    "            dsl_cls, obj_att = pcfg_encoder(x)\n",
    "            obj_att = obj_att[2:first_pad + 2]  # Since we have 2 special cls tokens at the start\n",
    "\n",
    "            # Compute individual losses\n",
    "            # dsl_loss, obj_loss = compute_kl_loss(\n",
    "            #     dsl_cls, \n",
    "            #     obj_att, \n",
    "            #     torch.tensor(label, device=device), \n",
    "            #     torch.tensor(obj_idx, device=device)\n",
    "            # )\n",
    "            dsl_loss, obj_loss = compute_cross_entropy_loss(\n",
    "                dsl_cls, \n",
    "                obj_att, \n",
    "                torch.tensor(label, device=device), \n",
    "                torch.tensor(obj_idx, device=device)\n",
    "            )\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            # total_loss = dsl_loss + obj_loss\n",
    "            total_loss = dsl_loss\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "\n",
    "        # Accumulate losses\n",
    "        total_loss_sum += total_loss.item()\n",
    "        dsl_loss_sum += dsl_loss.item()\n",
    "        obj_loss_sum += obj_loss.item()\n",
    "        step += 1\n",
    "\n",
    "        if step % train_params['print_frequency'] == 0:\n",
    "            avg_total_loss = total_loss_sum / train_params['print_frequency']\n",
    "            avg_dsl_loss = dsl_loss_sum / train_params['print_frequency']\n",
    "            avg_obj_loss = obj_loss_sum / train_params['print_frequency']\n",
    "\n",
    "            avg_losses.append((avg_total_loss, avg_dsl_loss, avg_obj_loss))\n",
    "\n",
    "            print(f\"[{(epoch+1):>2}/{train_params['epochs']:>2}] - {step:>4}: Total Loss = {avg_total_loss:.4f}, DSL Loss = {avg_dsl_loss:.4f}, Obj Loss = {avg_obj_loss:.4f}\")\n",
    "\n",
    "            total_loss_sum = 0\n",
    "            dsl_loss_sum = 0\n",
    "            obj_loss_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4062, -0.0432, -1.4668,  0.1714, -2.6191, -0.8252,  0.2338,  0.6240,\n",
      "         0.6084], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.1605, 0.1024, 0.0247, 0.1268, 0.0078, 0.0468, 0.1350, 0.1995, 0.1964],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SoftmaxBackward0>)\n",
      "tensor([ 0.0194,  0.0800, -0.0224,  0.0195,  0.0760,  0.0933], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Model and embedding model saved to trained_pcfg_models\\pcfg_encoder_20241204_091144.pth\n"
     ]
    }
   ],
   "source": [
    "print(dsl_cls)\n",
    "print(F.softmax(dsl_cls, dim=-1))\n",
    "print(obj_att)\n",
    "pcfg_encoder.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-291c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
