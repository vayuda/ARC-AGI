{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Iteration of Image Encoder\n",
    "---   \n",
    "Trying new type of loss - this time, only relying on NCE between transformations of the same image to try to get the model to distinguish between those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_60448\\2751531485.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()   # Train w/ Mixed Precision for faster training\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "import source as source\n",
    "import utility_v2 as util_v2\n",
    "import vision_transformer_v2 as vit_v2\n",
    "import v2_loader as pcfg_loader\n",
    "\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "scaler = GradScaler()   # Train w/ Mixed Precision for faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_params = {\n",
    "    'folder_path': 'data/training',\n",
    "    'only_inputs': True,\n",
    "    'percent_mask': 0.1,\n",
    "    'pad_images': True,\n",
    "    'transformation_depth': 2,\n",
    "    'transformation_samples': 4,\n",
    "    'p_use_base': 0.15,\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    \"epochs\": 25,\n",
    "    \"lr\": 1e-4,\n",
    "    \"print_frequency\": 50,\n",
    "    \"max_eval_iters\": None,\n",
    "    \"ema_alpha\": 0.01\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    \"max_img_size\": 32,\n",
    "    \"unique_patches\": 13,\n",
    "    \"embed_dim\": 16,\n",
    "    \"depth\": 8,\n",
    "    \"num_heads\": 2,\n",
    "    \"mlp_ratio\": 2,\n",
    "    \"qkv_bias\": False,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"attn_drop_rate\": 0.0,\n",
    "    \"pos_enc\": \"Sinusoidal\",\n",
    "    \"sinusoidal_theta\": 10000,\n",
    "    \"head\": False,\n",
    "    \"print_statements\": True,\n",
    "    \"output_classes\": 13,\n",
    "    \"device\": device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iBOT(student_ViT, teacher_ViT, loader_params, train_params, device):\n",
    "    csv_log_file = util_v2.setup_csv_logger()\n",
    "    _, train_dataloader = pcfg_loader.get_pcfg_datahandlers(**loader_params)\n",
    "    optim = torch.optim.AdamW(student_ViT.parameters(), lr=train_params['lr'])\n",
    "    \n",
    "    for epoch in range(train_params['epochs']):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{train_params['epochs']}\")\n",
    "        print(f\"{'Iter':>9} || {'Train Loss':>11} | {'Samples/s':>10}\")\n",
    "        train_loss, last_iter, train_samples = 0, 0, 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, (ids, u, u_masks, v, v_masks) in enumerate(train_dataloader):\n",
    "            source.plot_image_and_mask(u[0], u_masks[0])\n",
    "            return False\n",
    "            u, u_masks, v, v_masks = u.to(device), u_masks.to(device), v.to(device), v_masks.to(device)\n",
    "            train_samples += 1\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):  # Enable mixed precision\n",
    "                loss = util_v2.compute_loss(u, u_masks, v, v_masks, student_ViT, teacher_ViT)\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "            \n",
    "            util_v2.update_teacher_weights(student_ViT, teacher_ViT, train_params['ema_alpha'])\n",
    "    \n",
    "            train_loss += loss.cpu().item()\n",
    "            if (i+1) % train_params['print_frequency'] == 0 or (i+1) == len(train_dataloader):\n",
    "                iter_count = (i+1) - last_iter\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                train_loss /= iter_count\n",
    "                samples_per_second = ((iter_count * loader_params['transformation_samples']) / elapsed_time)\n",
    "                print(f\"{(i + 1):>4}/{len(train_dataloader):>4} || {train_loss:>11.3f} | {samples_per_second:>9.2f}\")\n",
    "\n",
    "                last_iter = (i+1)\n",
    "                start_time = time.time()\n",
    "                sample_num = train_samples + epoch * len(train_dataloader)\n",
    "                util_v2.log_to_csv(csv_log_file, sample_num, train_loss, samples_per_second)\n",
    "                train_loss = 0\n",
    "        \n",
    "        # student_ViT.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Transformer instantiated with 17,296 parameters using Sinusoidal encodings.\n",
      "Vision Transformer instantiated with 17,296 parameters using Sinusoidal encodings.\n",
      "\n",
      "Epoch 1/25\n",
      "     Iter ||  Train Loss |  Samples/s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate and initialize teacher network to match the student\n",
    "student_ViT = vit_v2.VisionTransformer(**model_params).to(device)\n",
    "teacher_ViT = vit_v2.VisionTransformer(**model_params).to(device)\n",
    "teacher_ViT.load_state_dict(student_ViT.state_dict())\n",
    "\n",
    "# Train model\n",
    "train_iBOT(student_ViT, teacher_ViT, loader_params, train_params, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc-291c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
